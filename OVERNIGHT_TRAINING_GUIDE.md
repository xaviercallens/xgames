# Overnight PPO Training Guide

**Complete guide for running long-duration PPO training sessions**

---

## ðŸŽ¯ Quick Start

### Option 1: Simple Start (Recommended)
```bash
# Start training in background with logging
nohup python3 overnight_training.py > training_output.log 2>&1 &

# Monitor progress in another terminal
python3 monitor_training.py
```

### Option 2: Using Screen (Persistent Session)
```bash
# Start a screen session
screen -S bomberman_training

# Run training
python3 overnight_training.py

# Detach: Press Ctrl+A then D
# Reattach: screen -r bomberman_training
```

### Option 3: Using tmux
```bash
# Start tmux session
tmux new -s training

# Run training
python3 overnight_training.py

# Detach: Press Ctrl+B then D
# Reattach: tmux attach -t training
```

---

## ðŸ“Š Features

### Automatic Checkpointing
- âœ… Saves every 100 episodes
- âœ… Autosaves every 30 minutes
- âœ… Saves best performing model
- âœ… Resume from checkpoint support

### Progress Tracking
- âœ… Real-time statistics logging
- âœ… Win rate monitoring
- âœ… Reward tracking
- âœ… Performance visualization

### Adaptive Learning
- âœ… Learning rate decay
- âœ… Early stopping on plateau
- âœ… Performance-based checkpointing

### Safety Features
- âœ… Graceful Ctrl+C handling
- âœ… Time limit enforcement
- âœ… Automatic state saving
- âœ… Error recovery

---

## âš™ï¸ Configuration

### Training Parameters

Edit `overnight_training.py` to customize:

```python
# Training duration
TOTAL_EPISODES = 10000  # Target episodes
TRAINING_HOURS = 8      # Maximum hours

# PPO Hyperparameters
UPDATE_INTERVAL = 4096  # Steps per update
LEARNING_RATE_START = 3e-4
LEARNING_RATE_END = 1e-5
CLIP_EPSILON = 0.2
GAMMA = 0.99

# Checkpointing
CHECKPOINT_INTERVAL = 100  # Episodes
AUTOSAVE_INTERVAL = 1800   # Seconds (30 min)
```

### Recommended Settings

**For Overnight (8 hours):**
```python
TOTAL_EPISODES = 10000
TRAINING_HOURS = 8
UPDATE_INTERVAL = 4096
```

**For Quick Test (1 hour):**
```python
TOTAL_EPISODES = 1000
TRAINING_HOURS = 1
UPDATE_INTERVAL = 2048
```

**For Weekend (48 hours):**
```python
TOTAL_EPISODES = 50000
TRAINING_HOURS = 48
UPDATE_INTERVAL = 8192
```

---

## ðŸ“ˆ Monitoring

### Real-time Monitoring

```bash
# Start monitor (refreshes every 30 seconds)
python3 monitor_training.py

# Custom refresh interval (60 seconds)
python3 monitor_training.py --interval 60
```

### Check Summary

```bash
# Print summary without continuous monitoring
python3 monitor_training.py --summary
```

### Generate Plots

```bash
# Generate performance plots
python3 monitor_training.py --plots

# Plots saved to: bomber_game/models/plots/
```

### Monitor Output

The monitor displays:
- Current episode and progress
- Win rate (last 100 episodes)
- Average reward
- Best win rate achieved
- Training speed (episodes/hour)
- Estimated time remaining
- Overall statistics
- Performance trends

---

## ðŸ“ Output Files

### Training Artifacts

```
bomber_game/models/
â”œâ”€â”€ ppo_agent.pth              # Final trained model
â”œâ”€â”€ training_stats.json        # Complete statistics
â”œâ”€â”€ overnight_progress.json    # Current progress
â”œâ”€â”€ training_log.txt          # Detailed log
â”œâ”€â”€ checkpoints/              # Training checkpoints
â”‚   â”œâ”€â”€ latest_checkpoint.pth
â”‚   â”œâ”€â”€ checkpoint_ep100_periodic_*.pth
â”‚   â”œâ”€â”€ checkpoint_ep200_best_*.pth
â”‚   â””â”€â”€ *.json (metadata)
â””â”€â”€ plots/                    # Performance plots
    â”œâ”€â”€ win_rate.png
    â”œâ”€â”€ avg_reward.png
    â”œâ”€â”€ episode_rewards.png
    â””â”€â”€ combined_progress.png
```

### File Descriptions

**ppo_agent.pth**
- Final trained model
- Use this for playing against the AI

**training_stats.json**
- Complete training history
- Episode rewards, win rates, etc.

**overnight_progress.json**
- Current training state
- Updated every 10 episodes

**training_log.txt**
- Timestamped log of all events
- Useful for debugging

**checkpoints/**
- Periodic saves during training
- Resume training from any checkpoint
- Includes metadata for each checkpoint

**plots/**
- Visual progress charts
- Generated by monitor script
- Updated automatically

---

## ðŸ”„ Resume Training

### Automatic Resume

The script automatically resumes from the latest checkpoint:

```bash
# Just run again - it will find and load the latest checkpoint
python3 overnight_training.py
```

### Manual Resume from Specific Checkpoint

Edit `overnight_training.py`:

```python
# In train_overnight() function, modify:
checkpoint_path = "bomber_game/models/checkpoints/checkpoint_ep500_best_*.pth"
agent = PPOAgent(agent_player, model_path=checkpoint_path, training=True)
```

---

## ðŸŽ® Using the Trained Model

### Play Against Trained AI

```bash
# The game automatically uses the best model
python3 play_bomberman.py
```

### Test Model Performance

```bash
# Run benchmark tests
python3 quick_test_agent.py
```

### Compare Models

```bash
# Compare different checkpoints
python3 -c "
from bomber_game.model_selector import ModelSelector
selector = ModelSelector('bomber_game/models')
print(selector.get_performance_report())
"
```

---

## ðŸ“Š Expected Results

### Training Timeline

**First Hour (0-1h):**
- Episodes: ~500-800
- Win Rate: 10-20%
- Learning basic movement and bomb placement

**Hours 2-4:**
- Episodes: 1,500-3,000
- Win Rate: 20-35%
- Developing strategic play

**Hours 5-8:**
- Episodes: 4,000-8,000
- Win Rate: 35-50%+
- Refined tactics and positioning

### Performance Targets

| Metric | Good | Excellent |
|--------|------|-----------|
| Win Rate (vs Heuristic) | 40%+ | 55%+ |
| Avg Reward | 50+ | 100+ |
| Episodes/Hour | 500+ | 800+ |

---

## ðŸ› ï¸ Troubleshooting

### Training Not Starting

```bash
# Check if PyTorch is installed
python3 -c "import torch; print(torch.__version__)"

# Install if missing
pip3 install torch
```

### Low Performance (Slow Training)

```bash
# Check CPU usage
top

# Reduce update interval for faster training
# Edit overnight_training.py:
UPDATE_INTERVAL = 2048  # Smaller = faster but less stable
```

### Out of Memory

```bash
# Reduce batch size
# Edit overnight_training.py:
BATCH_SIZE = 64  # Smaller = less memory
```

### Training Plateaus

The script has automatic plateau detection:
- Stops if no improvement for 50 episodes
- Saves best model before stopping
- Check logs for plateau warnings

### Resume Not Working

```bash
# Check for checkpoint files
ls -lh bomber_game/models/checkpoints/

# Manually specify checkpoint
# Edit overnight_training.py to load specific checkpoint
```

---

## ðŸ’¡ Tips for Best Results

### 1. System Preparation

```bash
# Ensure system won't sleep
sudo systemctl mask sleep.target suspend.target

# Or use caffeine/similar tools
```

### 2. Resource Allocation

- Close unnecessary applications
- Ensure adequate disk space (>1GB)
- Monitor temperature if on laptop

### 3. Optimal Settings

- Start with default settings
- Adjust based on first hour results
- Larger `UPDATE_INTERVAL` = more stable
- Smaller `LEARNING_RATE` = slower but safer

### 4. Monitoring Strategy

```bash
# Terminal 1: Training
nohup python3 overnight_training.py > training.log 2>&1 &

# Terminal 2: Monitor
python3 monitor_training.py

# Terminal 3: System resources
htop
```

### 5. Post-Training

```bash
# Generate final plots
python3 monitor_training.py --plots

# Print summary
python3 monitor_training.py --summary

# Test the model
python3 play_bomberman.py
```

---

## ðŸš€ Advanced Usage

### Custom Reward Function

Edit `calculate_reward()` in `overnight_training.py`:

```python
def calculate_reward(game_state, agent_player, enemy_player, prev_state, action):
    # Customize rewards here
    reward = 0
    
    # Your custom logic
    if custom_condition:
        reward += custom_value
    
    return reward
```

### Hyperparameter Tuning

Create a tuning script:

```python
# hyperparameter_search.py
learning_rates = [1e-4, 3e-4, 5e-4]
clip_epsilons = [0.1, 0.2, 0.3]

for lr in learning_rates:
    for clip in clip_epsilons:
        # Run training with these parameters
        # Compare results
```

### Multi-Agent Training

Train against multiple opponents:

```python
# Rotate opponents during training
opponents = [
    ImprovedHeuristicAgent,
    SimpleAgent,
    PPOAgent  # Self-play
]
```

---

## ðŸ“ž Support

### Check Training Status

```bash
# View last 50 lines of log
tail -n 50 bomber_game/models/training_log.txt

# Watch log in real-time
tail -f bomber_game/models/training_log.txt
```

### Common Issues

**Issue:** Training stops unexpectedly
**Solution:** Check `training_log.txt` for errors

**Issue:** Win rate not improving
**Solution:** Adjust reward function or learning rate

**Issue:** Memory errors
**Solution:** Reduce `BATCH_SIZE` and `UPDATE_INTERVAL`

---

## âœ… Checklist

Before starting overnight training:

- [ ] PyTorch installed (`pip3 install torch`)
- [ ] Matplotlib installed (`pip3 install matplotlib`) - optional
- [ ] Sufficient disk space (>1GB)
- [ ] System won't sleep/hibernate
- [ ] Configuration reviewed
- [ ] Monitor script tested

During training:

- [ ] Check progress after 1 hour
- [ ] Monitor system resources
- [ ] Verify checkpoints are saving

After training:

- [ ] Generate final plots
- [ ] Review training summary
- [ ] Test trained model
- [ ] Backup best checkpoint

---

## ðŸŽ‰ Success Metrics

Your training is successful if:

âœ… Win rate > 40% (vs heuristic baseline ~30%)
âœ… Consistent improvement over time
âœ… No crashes or errors
âœ… Checkpoints saved regularly
âœ… Model performs well in actual games

---

**Happy Training! ðŸš€**

For questions or issues, check the training log or review the code comments.
